{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61098e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.patches as mpatches\n",
    "import string\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299399f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned_dataset:\n",
    "df = pd.read_csv(r'C:\\Users\\HP\\Desktop\\Digi-crome\\Project-7_NLP\\Data\\cleaned_twitter_disaster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Structure\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8ae3d",
   "metadata": {},
   "source": [
    "# Task: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2dd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the relevant features:\n",
    "\n",
    "# Extract the word frequency features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the cleaned_text to get the word frequency matrix\n",
    "X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "\n",
    "# Extract the  TF-IDF scores features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the cleaned_text to get the TF-IDF matrix\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e41b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48b419",
   "metadata": {},
   "source": [
    "## Sentiments Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the sentiment analysis features\n",
    "from textblob import TextBlob\n",
    "cleaned_text=df['cleaned_text']\n",
    "\n",
    "# Function to perform sentiment analysis\n",
    "def get_sentiment(cleaned_text):\n",
    "    blob = TextBlob(cleaned_text)\n",
    "    \n",
    "    # Sentiment polarity (-1 to 1)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    \n",
    "    # Subjectivity (0 to 1)\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    \n",
    "    return polarity, subjectivity\n",
    "\n",
    "# Apply sentiment analysis to each text\n",
    "sentiments = [get_sentiment(cleaned_text) for cleaned_text in cleaned_text]\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "sentiment_df = pd.DataFrame(sentiments, columns=['Polarity', 'Subjectivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceddc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53156424",
   "metadata": {},
   "source": [
    "## Word Embeddings using GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f9e83",
   "metadata": {},
   "source": [
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus and the resulting representations showcase interesting linear substructures of the word vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= df[['text', 'cleaned_text']]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340306fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd94dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55073f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(data) for data in df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding cleaned_text data\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
    "padded_seq = pad_sequences(sequences, maxlen=131, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57219564",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding index\n",
    "embedding_index = {}\n",
    "\n",
    "# create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size+1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080de490",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d657b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3e896",
   "metadata": {},
   "source": [
    "## Additional Features: Hashtags, Mentions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate additional features\n",
    "def extract_additional_features(text):\n",
    "    num_hashtags = len(re.findall(r\"#\\w+\", text))\n",
    "    num_mentions = len(re.findall(r\"@\\w+\", text))\n",
    "    num_exclamations = len(re.findall(r\"!\\w+\", text))\n",
    "    return pd.Series([num_hashtags, num_mentions, num_exclamations], index=['num_hashtags', 'num_mentions', 'num_exclamations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to both training and test sets\n",
    "additional_future = df['text'].apply(extract_additional_features)\n",
    "additional_future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b8257",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_text']  \n",
    "y = df['target'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data into numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # Use the 5000 most frequent words\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41701d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes of the resulting datasets\n",
    "print(f\"Training data size: {X_train.shape}\")\n",
    "print(f\"Testing data size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894782f5",
   "metadata": {},
   "source": [
    "# Task: Model Selection and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99031e3e",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e5a7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Logistic Regression classifier\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "# Train the model using the training data\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83097744",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c95d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20663c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924da890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f8b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report (Precision, Recall, F1-Score)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ece303",
   "metadata": {},
   "source": [
    "## Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadeb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a9522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model = RandomForestClassifier()\n",
    "random_model.fit(X_train, y_train)\n",
    "y_pred_ran = random_model.predict(X_test)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "print(classification_report(y_test, y_pred_ran))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09eeca7",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing some important features\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NN_Model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_tfidf.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification (0 or 1)\n",
    "    \n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "# Wrap the Keras model into a scikit-learn compatible classifier\n",
    "nn_model  = model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model test accuracy\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bebe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d0e0c",
   "metadata": {},
   "source": [
    "# Optimize Hyperparameter Tuning Using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Logistic Regression Hyperparameter Tuning**\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "logistic_model = LogisticRegression()\n",
    "# Set up the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'], \n",
    "    'C': [0.01, 0.1, 1, 10, 100],                  \n",
    "    'solver': ['liblinear', 'saga'],               \n",
    "    'max_iter': [100, 200, 500]                    \n",
    "}\n",
    "# Set up GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(logistic_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d208cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec72190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model after tuning\n",
    "best_logistic_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the best model\n",
    "y_pred_log = best_logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Hyperparameter Tuning\n",
    "\n",
    "# Define the model\n",
    "random_model = RandomForestClassifier(random_state=42)\n",
    "# Set up the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],               \n",
    "    'max_depth': [None, 10, 20, 30],             \n",
    "    'min_samples_split': [2, 5, 10],              \n",
    "    'min_samples_leaf': [1, 2, 4],                \n",
    "    'max_features': ['sqrt', 'log2'],           \n",
    "    'bootstrap': [True, False]                  \n",
    "}\n",
    "# Set up GridSearchCV with cross-validation\n",
    "grid_search_ran = GridSearchCV(random_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba315c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search_ran.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f36423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model after tuning\n",
    "best_random_model = grid_search_ran.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bde09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the best model\n",
    "y_pred_ran = best_random_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Best Parameters:\", grid_search_ran.best_params_)\n",
    "print(classification_report(y_test, y_pred_ran))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807671cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Neural Network model Hyperparameter Tuning**\n",
    "\n",
    "def create_model():\n",
    "    model_neural_tue = Sequential()\n",
    "    model_neural_tue.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model_neural_tue.add(Dense(1))  # Single output for regression\n",
    "    model_neural_tue.compile(optimizer='adam', loss='mse')\n",
    "    return model_neural_tue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af530890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "model_neural_tue = KerasRegressor(model=create_model, epochs=10, batch_size=10, verbose=0)\n",
    "param_grid = {\n",
    "    'batch_size': [10, 20, 40],\n",
    "    'epochs': [10, 20, 50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=model_neural_tue, param_distributions=param_grid, n_iter=10, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20684d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the random search\n",
    "random_result = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = random_result.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e79f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
